#!/bin/bash

Trainer(Logger, [Callback])

Logger()
    TensorBoardLogger()

Callback()
    # TODO: EarlyStopper()
    LearningRateMonitor()
    ModelCheckpoint()

# TODO: design Result()

#####################################################################
# learner/, task/
Task(Network, Criterion, Optimizer, Scheduler, Regulerizer, Metrics)
    # TODO: SWA(), Augmix()
    Classification()
        Trainer.fit(Classification, DataLoader, DataLoader)
        Trainer.test(Classification, DataLoader) -> Result() path
    Segmentation()

#####################################################################
# vision/
DataLoader(Dataset, Sampler)

Dataset(Annotation, [Transform])
    # TODO: prefetch minibatch, preprocess
    ClassificationDataset()
    SegmentationDataset()

Annotation()
    SingleImageAnnotation()
    MultiImageAnnotation()
    PairImageAnnotation()

Transform()
    albumentations.DualTransform
    albumentations.ImageOnlyTransform

Sampler()
    RandomSampler()
    WeightedSampler()


#####################################################################
# visualizer/
GradCAM(Network, layer_name)
    Trainer.fit(GradCAM, DataLoader) -> Result() path
    # cannot use Trainer.test as we need backward gradient
    # and backward gradient can be acquired from training phase only
    # TODO: check again

ActivationMap(Network, layer_names)
    Trainer.fit(ActivationMap, DataLoader) -> Result() path
